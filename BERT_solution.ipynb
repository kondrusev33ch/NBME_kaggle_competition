{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e32418",
   "metadata": {},
   "source": [
    "**References**\n",
    "- [Evaluation metrics](https://www.kaggle.com/code/theoviel/evaluation-metric-folds-baseline#Metric)\n",
    "- [Bert](https://www.kaggle.com/code/tomohiroh/nbme-bert-for-beginners)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10622541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffba7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "SEED = 13\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b75172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>family history of mi; family history of myocar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>family history of thyroid disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>[chest pressure]</td>\n",
       "      <td>[203 217]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>chest pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>[intermittent episodes, episode]</td>\n",
       "      <td>[70 91, 176 183]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>intermittent symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[felt as if he were going to pass out]</td>\n",
       "      <td>[222 258]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>lightheaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>stephanie madden is a 20 year old woman compla...</td>\n",
       "      <td>family history of migraines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>stephanie madden is a 20 year old woman compla...</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>[photobia]</td>\n",
       "      <td>[274 282]</td>\n",
       "      <td>stephanie madden is a 20 year old woman compla...</td>\n",
       "      <td>photophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>[no sick contacts]</td>\n",
       "      <td>[421 437]</td>\n",
       "      <td>stephanie madden is a 20 year old woman compla...</td>\n",
       "      <td>no known illness contacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>[Subjective fever]</td>\n",
       "      <td>[314 330]</td>\n",
       "      <td>stephanie madden is a 20 year old woman compla...</td>\n",
       "      <td>subjective fever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                   annotation          location  \\\n",
       "0              [dad with recent heart attcak]         [696 724]   \n",
       "1                 [mom with \"thyroid disease]         [668 693]   \n",
       "2                            [chest pressure]         [203 217]   \n",
       "3            [intermittent episodes, episode]  [70 91, 176 183]   \n",
       "4      [felt as if he were going to pass out]         [222 258]   \n",
       "...                                       ...               ...   \n",
       "14295                                      []                []   \n",
       "14296                                      []                []   \n",
       "14297                              [photobia]         [274 282]   \n",
       "14298                      [no sick contacts]         [421 437]   \n",
       "14299                      [Subjective fever]         [314 330]   \n",
       "\n",
       "                                              pn_history  \\\n",
       "0      hpi: 17yo m presents with palpitations. patien...   \n",
       "1      hpi: 17yo m presents with palpitations. patien...   \n",
       "2      hpi: 17yo m presents with palpitations. patien...   \n",
       "3      hpi: 17yo m presents with palpitations. patien...   \n",
       "4      hpi: 17yo m presents with palpitations. patien...   \n",
       "...                                                  ...   \n",
       "14295  stephanie madden is a 20 year old woman compla...   \n",
       "14296  stephanie madden is a 20 year old woman compla...   \n",
       "14297  stephanie madden is a 20 year old woman compla...   \n",
       "14298  stephanie madden is a 20 year old woman compla...   \n",
       "14299  stephanie madden is a 20 year old woman compla...   \n",
       "\n",
       "                                            feature_text  \n",
       "0      family history of mi; family history of myocar...  \n",
       "1                     family history of thyroid disorder  \n",
       "2                                         chest pressure  \n",
       "3                                  intermittent symptoms  \n",
       "4                                            lightheaded  \n",
       "...                                                  ...  \n",
       "14295                        family history of migraines  \n",
       "14296                                             female  \n",
       "14297                                        photophobia  \n",
       "14298                          no known illness contacts  \n",
       "14299                                   subjective fever  \n",
       "\n",
       "[14300 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load prepaired training csv\n",
    "train_df = pd.read_csv('train_ready.csv',\n",
    "                       converters={'annotation': lambda x: literal_eval(x),\n",
    "                                   'location': lambda x: literal_eval(x)})\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b1455",
   "metadata": {},
   "source": [
    "# KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfbae04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14300, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add kfold markup\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "train_df['stratify_on'] = train_df['case_num'].astype(str) + train_df['feature_num'].astype(str)\n",
    "train_df['fold'] = -1\n",
    "\n",
    "for fold, (_, valid_idx) in enumerate(skf.split(train_df['id'], y=train_df['stratify_on'])):\n",
    "    train_df.loc[valid_idx, 'fold'] = fold\n",
    "    \n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d35b40",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390fb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init tokenizer\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7ca917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_list_to_ints(loc_list) -> list:\n",
    "    \"\"\" \n",
    "    Convert type of 'location' column to int.\n",
    "    \n",
    "    Args:\n",
    "        loc_list (list): list with locations.\n",
    "        \n",
    "    Returns:\n",
    "        list: converted to int results.\n",
    "    \"\"\"\n",
    "    \n",
    "    to_return = []\n",
    "    for loc_str in loc_list:\n",
    "        loc_strs = loc_str.split(';')\n",
    "        for loc in loc_strs:\n",
    "            start, end = loc.split(' ')\n",
    "            to_return.append((int(start), int(end)))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2786d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_add_labels(tokenizer_, row) -> dict:\n",
    "    \"\"\"\n",
    "    Tokenize and add labels.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer_ (transformers bert-base-uncased): tokenizer for the BERT model.\n",
    "        row (dict): one row from the dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        dict: tokenized data.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize input data, and get 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'\n",
    "    tokenized_inputs = tokenizer_(row['feature_text'],\n",
    "                                  row['pn_history'],\n",
    "                                  truncation = 'only_second',\n",
    "                                  max_length = 416,\n",
    "                                  padding = 'max_length',\n",
    "                                  return_offsets_mapping = True)\n",
    "    \n",
    "    # Create zero float list \n",
    "    labels = [0.0] * len(tokenized_inputs['input_ids']) \n",
    "    tokenized_inputs['location_int'] = loc_list_to_ints(row['location'])  # add converted to int locations\n",
    "    tokenized_inputs['sequence_ids'] = tokenized_inputs.sequence_ids()  # add sequence ids\n",
    "    \n",
    "    # Fill labels\n",
    "    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs['sequence_ids'], tokenized_inputs['offset_mapping'])):\n",
    "        # -100 is our fill value \n",
    "        if seq_id is None or seq_id == 0:\n",
    "            labels[idx] = -100.0\n",
    "            continue\n",
    "            \n",
    "        # Based on location, mark with 1 the righ position of the feature \n",
    "        exit = False\n",
    "        token_start, token_end = offsets\n",
    "        for feature_start, feature_end in tokenized_inputs['location_int']:\n",
    "            if exit:\n",
    "                break\n",
    "            if token_start >= feature_start and token_end <= feature_end:\n",
    "                labels[idx] = 1.0\n",
    "                exit = True\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2361781",
   "metadata": {},
   "source": [
    "### Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44758d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_text\n",
      "heart pounding; heart racing\n",
      "====================================================================================================\n",
      "pn_history\n",
      "hpi: patient is a 17 yo m with a c/o of palpitations.  palpitations began a few months ago. states that palpitations are sudden, unpredictable and feel like his heart is pounding fast/jumping out of his chest. typically these episodes last 3-4 minutes and resolve on their own. his most recent episode was 2 days ago and lasted about 10 minutes. during this epsiode he felt lightheaded, short of breath and had chest pressure located in the middle of his chest. denies any sweating, changes in hair or bowel movements.\r\n",
      "ros: negative except as stated above\r\n",
      "pmh: none\r\n",
      "meds: takes his roommates adderall to help study\r\n",
      "allergies: nkda\r\n",
      "pshx: none\r\n",
      "fh: mother has a thyroid problem, father had a mi this past year at age 53\r\n",
      "sh: denies to\n",
      "====================================================================================================\n",
      "location\n",
      "['40 52', '55 67', '104 116', '161 178', '161 169;179 183']\n",
      "====================================================================================================\n",
      "annotation\n",
      "['palpitations', 'Palpitations', 'palpitations', 'heart is pounding', 'heart is fast']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# This is how the data will look like in pipeline\n",
    "example_df = train_df.copy()\n",
    "\n",
    "first = example_df.loc[100]\n",
    "example = {'feature_text': first.feature_text,\n",
    "           'pn_history': first.pn_history,\n",
    "           'location': first.location,\n",
    "           'annotation': first.annotation}\n",
    "\n",
    "for key in example.keys():\n",
    "    print(key)\n",
    "    print(example[key])\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d791f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "[101, 2540, 9836, 1025, 2540, 3868, 102, 6522, 2072, 1024, 5776, 2003, 1037, 2459, 10930, 1049, 2007, 1037, 1039, 1013, 1051, 1997, 14412, 23270, 10708, 1012, 14412, 23270, 10708, 2211, 1037, 2261, 2706, 3283, 1012, 2163, 2008, 14412, 23270, 10708, 2024, 5573, 1010, 21446, 1998, 2514, 2066, 2010, 2540, 2003, 9836, 3435, 1013, 8660, 2041, 1997, 2010, 3108, 1012, 4050, 2122, 4178, 2197, 1017, 1011, 1018, 2781, 1998, 10663, 2006, 2037, 2219, 1012, 2010, 2087, 3522, 2792, 2001, 1016, 2420, 3283, 1998, 6354, 2055, 2184, 2781, 1012, 2076, 2023, 20383, 3695, 3207, 2002, 2371, 2422, 4974, 2098, 1010, 2460, 1997, 3052, 1998, 2018, 3108, 3778, 2284, 1999, 1996, 2690, 1997, 2010, 3108, 1012, 23439, 2151, 18972, 1010, 3431, 1999, 2606, 2030, 6812, 2884, 5750, 1012, 20996, 2015, 1024, 4997, 3272, 2004, 3090, 2682, 7610, 2232, 1024, 3904, 19960, 2015, 1024, 3138, 2010, 18328, 2015, 5587, 21673, 2140, 2000, 2393, 2817, 2035, 2121, 17252, 1024, 25930, 2850, 8827, 2232, 2595, 1024, 3904, 1042, 2232, 1024, 2388, 2038, 1037, 29610, 3291, 1010, 2269, 2018, 1037, 2771, 2023, 2627, 2095, 2012, 2287, 5187, 14021, 1024, 23439, 2000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "====================================================================================================\n",
      "token_type_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "====================================================================================================\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "====================================================================================================\n",
      "offset_mapping\n",
      "[(0, 0), (0, 5), (6, 14), (14, 15), (16, 21), (22, 28), (0, 0), (0, 2), (2, 3), (3, 4), (5, 12), (13, 15), (16, 17), (18, 20), (21, 23), (24, 25), (26, 30), (31, 32), (33, 34), (34, 35), (35, 36), (37, 39), (40, 43), (43, 46), (46, 52), (52, 53), (55, 58), (58, 61), (61, 67), (68, 73), (74, 75), (76, 79), (80, 86), (87, 90), (90, 91), (92, 98), (99, 103), (104, 107), (107, 110), (110, 116), (117, 120), (121, 127), (127, 128), (129, 142), (143, 146), (147, 151), (152, 156), (157, 160), (161, 166), (167, 169), (170, 178), (179, 183), (183, 184), (184, 191), (192, 195), (196, 198), (199, 202), (203, 208), (208, 209), (210, 219), (220, 225), (226, 234), (235, 239), (240, 241), (241, 242), (242, 243), (244, 251), (252, 255), (256, 263), (264, 266), (267, 272), (273, 276), (276, 277), (278, 281), (282, 286), (287, 293), (294, 301), (302, 305), (306, 307), (308, 312), (313, 316), (317, 320), (321, 327), (328, 333), (334, 336), (337, 344), (344, 345), (346, 352), (353, 357), (358, 361), (361, 363), (363, 365), (366, 368), (369, 373), (374, 379), (379, 383), (383, 385), (385, 386), (387, 392), (393, 395), (396, 402), (403, 406), (407, 410), (411, 416), (417, 425), (426, 433), (434, 436), (437, 440), (441, 447), (448, 450), (451, 454), (455, 460), (460, 461), (462, 468), (469, 472), (473, 481), (481, 482), (483, 490), (491, 493), (494, 498), (499, 501), (502, 505), (505, 507), (508, 517), (517, 518), (520, 522), (522, 523), (523, 524), (525, 533), (534, 540), (541, 543), (544, 550), (551, 556), (558, 560), (560, 561), (561, 562), (563, 567), (569, 572), (572, 573), (573, 574), (575, 580), (581, 584), (585, 593), (593, 594), (595, 598), (598, 602), (602, 603), (604, 606), (607, 611), (612, 617), (619, 622), (622, 624), (624, 628), (628, 629), (630, 632), (632, 634), (636, 638), (638, 639), (639, 640), (640, 641), (642, 646), (648, 649), (649, 650), (650, 651), (652, 658), (659, 662), (663, 664), (665, 672), (673, 680), (680, 681), (682, 688), (689, 692), (693, 694), (695, 697), (698, 702), (703, 707), (708, 712), (713, 715), (716, 719), (720, 722), (724, 726), (726, 727), (728, 734), (735, 737), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
      "====================================================================================================\n",
      "location_int\n",
      "[(40, 52), (55, 67), (104, 116), (161, 178), (161, 169), (179, 183)]\n",
      "====================================================================================================\n",
      "sequence_ids\n",
      "[None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "====================================================================================================\n",
      "labels\n",
      "[-100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\n",
    "for key in tokenized_inputs.keys():\n",
    "    print(key)\n",
    "    print(tokenized_inputs[key])\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e508de0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd778c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMEData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.loc[idx]\n",
    "        tokenized = tokenize_and_add_labels(self.tokenizer, sample)\n",
    "        \n",
    "        input_ids = np.array(tokenized['input_ids'])  # for input BERT\n",
    "        attention_mask = np.array(tokenized['attention_mask'])  # for input BERT\n",
    "        labels = np.array(tokenized['labels'])\n",
    "        \n",
    "        offset_mapping = np.array(tokenized['offset_mapping'])  \n",
    "        sequence_ids = np.array(tokenized['sequence_ids']).astype('float16') \n",
    "        \n",
    "        return input_ids, attention_mask, labels, offset_mapping, sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06bb64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMETestData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.loc[idx]\n",
    "        tokenized = self.tokenizer(sample['feature_text'],\n",
    "                                   sample['pn_history'],\n",
    "                                   truncation = 'only_second',\n",
    "                                   max_length = 416,\n",
    "                                   padding = 'max_length',\n",
    "                                   return_offsets_mapping=True)\n",
    "        tokenized['sequence_ids'] = tokenized.sequence_ids()\n",
    "\n",
    "        input_ids = np.array(tokenized['input_ids'])\n",
    "        attention_mask = np.array(tokenized['attention_mask'])\n",
    "        offset_mapping = np.array(tokenized['offset_mapping'])\n",
    "        sequence_ids = np.array(tokenized['sequence_ids']).astype('float16')\n",
    "\n",
    "        return input_ids, attention_mask, offset_mapping, sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02445d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMEModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(MODEL_NAME) # BERT model\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.classifier = torch.nn.Linear(768, 1) \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden_state = self.backbone(input_ids=input_ids, \n",
    "                                          attention_mask=attention_mask)[0] \n",
    "        logits = self.classifier(self.dropout(last_hidden_state)).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b0575",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0482f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca15b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = NBMEModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "test_folds = (0,)\n",
    "train_folds = (1, 2, 3, 4, 5, 6, 7)\n",
    "valid_folds = (8, 9)\n",
    "\n",
    "# Split training data frame into test, train and valid data frames\n",
    "test = train_df.loc[train_df['fold'] == test_folds[0]]\n",
    "for fold in test_folds[1:]:\n",
    "    test = test.append(train_df.loc[train_df['fold'] == fold]) \n",
    "test.reset_index(inplace=True, drop=True)    \n",
    "        \n",
    "train = train_df.loc[train_df['fold'] == train_folds[0]]\n",
    "for fold in train_folds[1:]:\n",
    "    train = train.append(train_df.loc[train_df['fold'] == fold])\n",
    "train.reset_index(inplace=True, drop=True)  \n",
    "\n",
    "valid = train_df.loc[train_df['fold'] == valid_folds[0]]\n",
    "for fold in valid_folds[1:]:\n",
    "    valid = valid.append(train_df.loc[train_df['fold'] == fold])\n",
    "valid.reset_index(inplace=True, drop=True)  \n",
    "\n",
    "# Init datasets    \n",
    "test_ds = NBMETestData(test, tokenizer)    \n",
    "train_ds = NBMEData(train, tokenizer)\n",
    "valid_ds = NBMEData(valid, tokenizer)\n",
    "\n",
    "# Init data loaders\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, \n",
    "                                      batch_size=BATCH_SIZE * 2,\n",
    "                                      pin_memory=True, \n",
    "                                      shuffle=False, \n",
    "                                      drop_last=False)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       pin_memory=True, \n",
    "                                       shuffle=True, \n",
    "                                       drop_last=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds,\n",
    "                                       batch_size=BATCH_SIZE * 2, \n",
    "                                       pin_memory=True, \n",
    "                                       shuffle=False, \n",
    "                                       drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e5c7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(t_dataloader, model_, optimizer_) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t_dataloader (torch.utils.data.DataLoader): training data loader.\n",
    "        model_ (bert-base-uncased): in this case it is BERT model.\n",
    "        optimizer_ (torch.optim.{}): torch.optim.AdamW for example.\n",
    "        \n",
    "    Returns:\n",
    "        float: average training loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_.train()\n",
    "    t_loss = AverageMeter()\n",
    "    \n",
    "    progress_bar = tqdm(t_dataloader)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[0].to(DEVICE)\n",
    "        attention_mask = batch[1].to(DEVICE)\n",
    "        labels = batch[2].to(DEVICE)\n",
    "        \n",
    "        logits = model_(input_ids, attention_mask)\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        loss = loss_fct(logits, labels)\n",
    "        loss = torch.masked_select(loss, labels > -1).mean() \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_.step()\n",
    "        t_loss.update(val=loss.item(), n=len(input_ids))\n",
    "        progress_bar.set_postfix(Loss=t_loss.avg)\n",
    "    \n",
    "    return t_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a74c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(v_dataloader, model_) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        v_dataloader (torch.utils.data.DataLoader): validation data loader.\n",
    "        model_ (bert-base-uncased): in this case it is BERT model.\n",
    "        \n",
    "    Returns:\n",
    "        float: average validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_.eval()\n",
    "    v_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        process_bar = tqdm(v_dataloader)\n",
    "        for batch in process_bar:\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "            labels = batch[2].to(DEVICE)\n",
    "            \n",
    "            logits = model_(input_ids, attention_mask)\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "            loss = loss_fct(logits, labels)\n",
    "            loss = torch.masked_select(loss, labels > -1).mean()\n",
    "    \n",
    "            v_loss.update(val=loss.item(), n=len(input_ids))\n",
    "            process_bar.set_postfix(Loss=v_loss.avg)\n",
    "            \n",
    "    return v_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2dcf9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "\n",
    "# Training process\n",
    "history = {'train': [], 'valid': []}\n",
    "if TRAIN:\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, EPOCHS))\n",
    "        \n",
    "        train_loss = train_fn(train_dl, model, optimizer)\n",
    "        history['train'].append(train_loss)\n",
    "\n",
    "        valid_loss = valid_fn(valid_dl, model)\n",
    "        history['valid'].append(valid_loss)\n",
    "\n",
    "        # Save model\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'nbme.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aad62",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d174d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "        \n",
    "    Example:\n",
    "        preds = [[0, 0, 1], [0, 0, 0]]\n",
    "        truths = [[0, 0, 1], [1, 0, 0]]\n",
    "        >>> 0.6666666666\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None) -> np.array:\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "        \n",
    "    Example:\n",
    "        spans = [(0, 5), (10, 15)]\n",
    "        >>> array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
    "    \"\"\"\n",
    "    \n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "        \n",
    "    Example:\n",
    "        preds = [[(0, 1)], [(2, 3), (4, 5)], [(6, 7)], [(10, 11), (12, 13), (14, 15)]]\n",
    "        truths = [[(0, 1)], [(2, 3), (4, 5)], [(6, 7)], [(8, 11), (12, 13), (14, 15)]]\n",
    "        >>> 0.8750000000000001\n",
    "    \"\"\"\n",
    "    \n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def create_spans_for_scoring(locations) -> list:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        locations (pd.Series): raw locations \n",
    "    \n",
    "    Example:\n",
    "        locations = [['0 1'], ['2 3', '4 5'], ['6 7'], [], ['10 11;12 13', '14 15']]\n",
    "        >>> [[(0, 1)], [(2, 3), (4, 5)], [(6, 7)], [], [(10, 11), (12, 13), (14, 15)]]\n",
    "    \"\"\"\n",
    "    \n",
    "    spans = []\n",
    "    for row in locations:\n",
    "        span = []\n",
    "        for loc_c in row:\n",
    "            for loc in loc_c.split(';'):\n",
    "                s, e = loc.split(' ')\n",
    "                span.append((int(s), int(e)))\n",
    "        spans.append(span)\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab5b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Standard sigmoid function.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def get_location_predictions(predictions, offset_mapping, sequence_ids, for_submission=False) -> list:\n",
    "    \"\"\"\n",
    "    Convert raw predictions to understandable location points.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): model output.\n",
    "        offset_mapping (list): tokenizer product.\n",
    "        sequence_ids (list): tokenizer product.\n",
    "        for_submission (bool): set True if converting predictions fro submission.\n",
    "        \n",
    "    Returns:\n",
    "        list: converted locations.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_predictions = []\n",
    "    for preds, offsets, seq_ids in zip(predictions, offset_mapping, sequence_ids):\n",
    "        preds = sigmoid(preds)\n",
    "        start_idx = None\n",
    "        \n",
    "        current_preds = []\n",
    "        for p, o, s_id in zip(preds, offsets, seq_ids):\n",
    "            if not s_id:\n",
    "                continue\n",
    "            if p > 0.5:\n",
    "                if start_idx is None:\n",
    "                    start_idx = o[0]\n",
    "                end_idx = o[1]\n",
    "            elif start_idx is not None:\n",
    "                if for_submission:\n",
    "                    current_preds.append(f'{start_idx} {end_idx}')\n",
    "                else:\n",
    "                    current_preds.append((start_idx, end_idx))\n",
    "                start_idx = None\n",
    "        if for_submission:\n",
    "            all_predictions.append('; '.join(current_preds))\n",
    "        else:\n",
    "            all_predictions.append(current_preds)\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85c11356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3b5c363b9d47668c578247eaf58975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('nbme.pth', map_location=DEVICE))\n",
    "\n",
    "def get_predictions(model_, dataloader, for_submission=False) -> list:\n",
    "    \"\"\"\n",
    "    Input test data into the model, convert outputs and return ready results.\n",
    "    \n",
    "    Args:\n",
    "        model_ (bert-base-uncased): in this case it is BERT model.\n",
    "        dataloader (torch.utils.data.DataLoader): data loader.\n",
    "        for_submission (bool): flag for get_location_predictions() function.\n",
    "        \n",
    "    Returns:\n",
    "        float: converted locations.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_.eval()\n",
    "    preds = []\n",
    "    offsets = []\n",
    "    seq_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl):\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "            offset_mapping = batch[2]\n",
    "            sequence_ids = batch[3]\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds.append(logits.cpu().numpy())\n",
    "            offsets.append(offset_mapping.numpy())\n",
    "            seq_ids.append(sequence_ids.numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    offsets = np.concatenate(offsets, axis=0)\n",
    "    seq_ids = np.concatenate(seq_ids, axis=0)\n",
    "    return get_location_predictions(preds, offsets, seq_ids, for_submission=for_submission) # convert predicted locations to spans\n",
    "\n",
    "pred_spans = get_predictions(model, test_dl)\n",
    "truth_df = test['location']  # get true locations from the test dataset \n",
    "truth_spans = create_spans_for_scoring(truth_df)  # convert locations to spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e722d21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8219291014014839"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model score\n",
    "span_micro_f1(pred_spans, truth_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3730e8",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c65dbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates test dataframe for submission.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: merged and preprocessed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    feats = pd.read_csv('./data/features.csv')\n",
    "    notes = pd.read_csv('./data/patient_notes.csv')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "    merged = test.merge(notes, how='left')\n",
    "    merged = merged.merge(feats, how='left')\n",
    "\n",
    "    def process_feature_text(text):\n",
    "        # Add here new prepocessing functions\n",
    "        return text.replace('-OR-', ';-').replace('-', ' ')\n",
    "    \n",
    "    merged['feature_text'] = [process_feature_text(x) for x in merged['feature_text']]\n",
    "    \n",
    "    print(merged.shape)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a523a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121e8be7a2c1429e91cfaea869d6fe2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>696 699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>668 693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>203 217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>70 91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id location\n",
       "0  00016_000  696 699\n",
       "1  00016_001  668 693\n",
       "2  00016_002  203 217\n",
       "3  00016_003    70 91\n",
       "4  00016_004      NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = create_test_df()\n",
    "test_ds = NBMETestData(test_df, tokenizer)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, \n",
    "                                      batch_size=BATCH_SIZE * 2, \n",
    "                                      pin_memory=True, \n",
    "                                      shuffle=False, \n",
    "                                      drop_last=False)\n",
    "\n",
    "location_preds = get_predictions(model, test_dl, for_submission=True)\n",
    "test_df['location'] = location_preds\n",
    "test_df[['id', 'location']].to_csv('bert_submission.csv', index=False)\n",
    "pd.read_csv('bert_submission.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbef65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
